\documentclass[10pt,peerreview]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\newcommand{\tabref}[2][]{Table#1~\ref{#2}\xspace}
\newcommand{\figref}[2][]{Figure#1~\ref{#2}\xspace}
\newcommand{\secref}[2][]{Section#1~\ref{#2}\xspace}
\begin{document}

\title{Automated Synthetic Image Data Generation \\with Annotations\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in explore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Mohammed Sharukh Syed} \qquad
\IEEEauthorblockN{Navnita Nandakumar}
\qquad
\IEEEauthorblockN{Saksham Agrawal} \\
\IEEEauthorblockA{\textit{Department of Computing and Information Systems} \\
\textit{The University of Melbourne}\\
Australia \\
\qquad {\tt \{syedm1|nnandakumar|sakshama\}@student.unimelb.edu.au}}}

%\maketitle
\IEEEpeerreviewmaketitle

\begin{abstract}
The process of training artificial intelligence systems requires huge datasets with manual annotations, which are expensive and time consuming to curate. With respect to image processing in particular, tagging objects in an image requires a wide range of training data, with instances depicting the annotated objects. In this paper, we propose a novel technique to generate virtually infinite number of  images of varying resolutions from a small dataset of polygon objects and desired backgrounds. We also provide automatically generated annotations for the objects within the images to eliminate the need for manual annotation. Our results, when compared with the state-of-the-art in playing card detection, display a significant increase in overall performance and accuracy. Furthermore, our image generator incorporates augmentation techniques with varying parameters to depict and annotate overlapping polygon objects according to the human perception of multiple objects in the line of sight. The synthetic data generated by this model can be used for the continuous evaluation of specialized artificial intelligence models. 
\end{abstract}

\section{Introduction}
While the identification of canonical views of individual categories have gained traction, generic object recognition and classification is still an unsolved, but widely researched task in Computer Vision. In general, Artificial Intelligence systems need to be trained on huge amounts of annotated data in order to make reliable predictions. Data curation is a time and cost consuming task in itself, but their manual annotation requires its own huge expense. Annotations by humans are also prone to error in individual judgement and personal bias. This isn't ideal as the quality of the AI system is dependent on the underlying quality of the data used to train it.

In this paper, we propose a means to synthetically generate data and their associated annotations, which we believe would provide functional value to any application that relies on predictions made by AI systems, as it reduces the time and cost required to procure training data considerably. Our consistent and reproducible annotations further improve the quality of any AI system trained on the generated data in terms of accuracy.

Our contributions are as follows:
\begin{enumerate}
    \item A synthetic image data generator that can create a large number of images from a handful of training instances
    \item Each generated image is accompanied by an XML annotation file, which can be used to automatically evaluate the AI system in training
    \item We propose the concept of polygon cropping to mimic the human perception of object overlays.
\end{enumerate}
Our last contribution is particularly valid because it allows AI systems to recognize cropped versions of objects as well (a card that is more than partially covered can still be recognized, for instance).

\section{Background}
State-of-the-art AI systems are trained using human generated and annotated data. Manually collecting and labelling data, however, is an expensive ordeal-- both in terms of time and cost. AI systems require huge amounts of data for each concept they are expected to learn and therefore, continuing to rely on human curated data will stall the growth and improvement of AI.

Direct manual annotation is another time consuming and labour intensive task, proportional to the level of detail required. Even with the employment of domain experts, we risk the possible inconsistency between annotators owing to bias and errors in judgement. Even if the 'quantity over quality' method is favoured, the cost of organising an event wherein a large number of people participate in the annotation has its own overheads involving organisation and motivation, as well as opening up to the high possibility of ``junk'' annotations. Another issue with manual annotations is that data is annotated with a specific task in mind. The annotations might not be as relevant for a different, but highly related task. The conversion of existing annotations is again a time consuming and expensive process\cite{Allan:08}.

Existing image annotation algorithms can be roughly divided into two categories\cite{Allan:08}-- \textit{model-based learning} methods and \textit{database-based retrieval} methods. Model-based methods explore the relationship between high-level semantic concepts and low-level visual features to discover a mapping function through machine learning or knowledge models for image annotation. Database-based methods, on the other hand, do not require any mapping function. Instead, they directly provide a sequence of candidate labels according to the annotated images that already exist in the database\cite{Hong:18}.

State-of-the-art synthetic image data generation concepts like \textit{Greppy Metaverse}\footnote{\url{http://metaverse.greppy.co/}} exist to generate photorealistic CAD models and provide new tools for higher-level image editing, e.g., adding objects to images or changing the appearance of existing objects. However, we must note the difficulty of generating ultra high-resolution images with GANs\cite{Ting:18}, as well as the fact that AI systems trained solely on high-resolution images cannot be expected to fare well over real world data that vary in resolution. The AI system should ideally be able to detect and recognize objects in low quality, noisy images as well.

\section{Experiments}
We decided to restrict our scope to the domain of playing cards, because of their relatively simple closed-polygon shape, as well as the limitation of validity. As synthetically generated datasets have the freedom of manipulation, it is paramount to restrict its technical boundaries in order to ensure the generated data is realistic and relevant in its scope. This required thorough analysis and fine-tuning through multiple experimental iterations. The iterations also allowed us to compare the computational costs for the different kinds of augmentation techniques used when generating the images. The evaluation of these computational costs further helped fully automate the model fully in its endeavour to generate valid datasets.

\includegraphics[width=3in]{proj_flow.png}
\captionof{figure}{Overview of the system}

For an AI system to accurately recognize a \textit{2 o Clubs} card in any image, it is trained on many images containing the \textit{2 of Clubs} in various angles, lighting and quality. Our goal is therefore to be able to detect the \textit{2 of Clubs} card object in a training instance, and apply augmentation techniques before superimposing it over a background to generate multiple training images from a single instance.

\subsection{Datasets}
We used 52 videos, one of each standard playing card against a simple background. Noise packs, backgrounds and card backs were sourced from the public domain, although our model also supports user-defined versions of these datasets. We extract frames from the videos and use an alpha mask to extract the card of standard dimensions.

\subsection{Object extraction}
Object extraction is the process of extracting the part of the image that is relevant to our problem domain (in this case the actual card i.e. we neglect the background and other items in the image). We use the alpha masking and image clipping properties from OpenCV to detect objects in our training data.
\newline
\includegraphics[width=2.5in]{2c.png}
\captionof{figure}{A frame extracted from training video instance}
\includegraphics[width=2.5in,height=2.5in]{extracted_card.png}
\captionof{figure}{Card extracted from frame using alpha mask}

Our alpha masks automatically form a bounding box using the dimensions extracted. Visualisation of the dimensions can be done using labelImg to confirm the annotations of the generated data.

\subsection{Image Augmentation}
We perform image augmentation techniques like adding noise and blurs, adjusting the lighting and rotating, sheering and translating the object and resulting image. This helps us mimic the nuances in data that occur in real life scenarios and allows us to create multiple variants of the same data objects to better train AI systems. Since the parameters can be varied, intense scenarios (like extreme \textit{Salt and Pepper noise}, for example) can be introduced to automatically evaluate the system's accuracy and capability to handle situations when faced with unknown noise.

\subsection{Scene Generation}
When humans perceive a scene containing multiple objects, often times the objects are overlayed and hide parts of the objects behind them. Thus, only partial objects are visible in the line of sight. However, as humans, we are able to perceive the existence of (and furthermore, recognize) the partially hidden object despite only viewing a part of it. Multi-polygon cropping helps mimic that observance in our artificially generated images. Consider, for example, a simple two-dimensional rectangle overlapped slightly by a circle (an image of a coin being placed partially over a monetary note, say). Instead of maintaining the rectangular polygon of the note, we consider its edges as now pertaining to the rectangle with a chunk pertaining to the extent of overlap with the circle removed. The resultant complex polygon is now depictive of the note. Training over such annotated polygons can help an AI system recognize even a folded note, say.

The core idea of our synthetic card data generator is to create a single artificial image using one or more images, while still preserving the positional data of the object that is required for training the AI system. The main challenge was to implement multiple objects and preserve their data in a manner similar to that of human perception, which involved the concept of handling multiple polygons and cropped polygons using the image augmentation techniques of computer vision.

Generally, AI models are trained with single object data in various forms, but in the real world, the objects are often overlapped with different entities and so training using overlapping data is essential to generate robust AI models that can detect and recognize the objects in question in varying real life scenarios.

The resolution of the image can be randomized and the generated datasets can, therefore can have scene images in different resolutions. This is not often observed in the case of manually annotated data sets. The resolution variance can further assist in AI training\footnote{\url{https://www.researchgate.net/publication/330339040\_Impact\_of\_Low\_Resolution\_on\_Image\_Recognition\_with\_Deep\_Neural\_Networks\_An\_Experimental\_Study}} as discussed.

\subsection{Results}
When the objects are stacked to form the scene image, the polygon data generated is recorded in VOC XML format which serves as readily available annotations for the AI models being trained. This annotation format can also be converted to other standard models with ease. We were able to generate up to 20000 images with upto 8 card objects each overnight using a standard laptop and 52 instances of training data.
\includegraphics[width=2.5in]{gen_img.png}
\captionof{figure}{An Example of the Image Generated (with noise)}
\includegraphics[width=2.5in]{xml_Annon.png}
\captionof{figure}{An Example of the Annotation file}

\section{Observations and Evaluation}
When we compared the performance of a naive card recognizing AI system\footnote{\url{https://github.com/geaxgx/playing-card-detection}} that was originally training on 200 human generated training images after training it on our synthetic dataset of 5000 card images, we found a 5.03\% increase in accuracy. This supports our claim that greater number of training instances helps improve the performance of the trained AI systems. We also report annotation accuracy of 98.2\% over generated data when compared with manual labelling.

The behaviour of the system was evaluated in a case-to-case scenario and documented for future use. It is noteworthy that objects sometimes tend to exceed image boundaries when generating large amounts of data due to the poor garbage handling techniques of the underlying libraries used. While these errors in the framework are extended to our project, further fine-tuning can help eliminate them.
\newline
\includegraphics[width=2.5in]{IDcard.png}
\captionof{figure}{Card Detection by geaxgx}
\includegraphics[width=2.5in]{without_crop.png}
\captionof{figure}{Object Edges without Polygon Cropping}
\subsection{Passive Test Oracle}
Since we provide both the images and their annotations, our generated datasets can also be used as passive test oracles to evaluate AI systems, in addition to training them. Since the datasets generated are different each time, the evaluation is continuous, consistent and automatic.

\subsection{Monitoring Computation Costs}
The time complexity of our system is measured to be \textbf{O(K*n\textsuperscript{m})}, where K is the number of scenes to be generated and m is number of objects per scene (this could also be a random choice (e.g. 1-8 objects per image)).

\section{Conclusion and Future Work}
In this paper, we presented a novel technique to generate multiple image data from a small training set of videos. The resultant data can incorporate up to 8 tested objects, presented with various augmentations like lighting, noise, blur, sheer, translation and transition.

Although our study is limited to playing cards, it can be extended to any rectangular shaped object of fixed dimensions. The idea of polygon edge detection and multi-polygon cropping itself, however, can be extended to any shaped object as any object can be decomposed into a finite sided polygon. Thus, this concept can be utilized to create a generalized object detection method.

\begin{thebibliography}{00}
\bibitem{Bied:87} Biederman, I. (1987). Recognition-by-components: a theory of human image understanding. Psychological review, 94(2), 115.
\bibitem{Abra:05} Abramson, Y., \& Freund, Y. (2005). Semi-automatic visual learning (seville): a tutorial on active learning for visual object recognition. In Intl. Conf. on Computer Vision and Pattern Recognition (CVPR05), San Diego.
\bibitem{Fei:07} Fei-Fei, L., Fergus, R., \& Perona, P. (2007). Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer vision and Image understanding, 106(1), 59-70.
\bibitem{Hoiem:08} Hoiem, D., Efros, A. A., \& Hebert, M. (2008). Putting objects in perspective. International Journal of Computer Vision, 80(1), 3-15.
\bibitem{Lecun:04} LeCun, Y., Huang, F. J., \& Bottou, L. (2004, June). Learning methods for generic object recognition with invariance to pose and lighting. In CVPR (2) (pp. 97-104).
\bibitem{Leibe:04} Leibe, B., Leonardis, A., \& Schiele, B. (2004). Combined object categorization and segmentation with an implicit shape model. In ECCV. 
\bibitem{Leibe:03} Leibe, B., \& Schiele, B. (2003, June). Analyzing appearance and contour based methods for object categorization. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings. (Vol. 2, pp. II-409). IEEE.
\bibitem{Leibe:04} Leibe, B., Leonardis, A., \& Schiele, B. (2004, May). Combined object categorization and segmentation with an implicit shape model. In Workshop on statistical learning in computer vision, ECCV (Vol. 2, No. 5, p. 7).
\bibitem{Opelt:06} Opelt, A., Pinz, A., \& Zisserman, A. (2006, May). A boundary-fragment-model for object detection. In European conference on computer vision (pp. 575-588). Springer, Berlin, Heidelberg.
\bibitem{Allan:08} Allan Hanbury. 2008. A survey of methods for image annotation. J. Vis. Lang. Comput. 19, 5 (October 2008), 617-627. DOI=http://dx.doi.org/10.1016/j.jvlc.2008.01.002 
\bibitem{Ting:18} Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz \& Bryan Catanzaro. 2018. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. NVIDIA Corporation, UC Berkeley In CVPR.
\bibitem{Hong:18} Hongwei Ge, Zehang Yan, Jing Dou, Zhen Wang, and ZhiQiang Wang, “A Semisupervised Framework for Automatic Image Annotation Based on Graph Embedding and Multiview Nonnegative Matrix Factorization,” Mathematical Problems in Engineering, vol. 2018, Article ID 5987906, 11 pages, 2018. https://doi.org/10.1155/2018/5987906.
\bibitem{} Y. Xiang, X. Zhou, T.-S. Chua, and C.-W. Ngo, “A revisit of generative model for automatic image annotation using markov random fields,” in Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009, pp. 1153–1160, June 2009.
\bibitem{}A. Makadia, V. Pavlovic, and S. Kumar, “A New Baseline for Image Annotation,” in Proceedings of the European Conference on Computer Vision, pp. 316–329, 2008.
\bibitem{} R. Rad and M. Jamzad, “Automatic image annotation by a loosely joint non-negative matrix factorisation,” IET Computer Vision, vol. 9, no. 6, pp. 806–813, 2015.
\bibitem{} M. M. Kalayeh, H. Idrees, and M. Shah, “NMF-KNN: image annotation using weighted multi-view non-negative matrix factorization,” in Proceedings of the 27th IEEE Conference on Computer Vision and Pattern Recognition (CVPR '14), pp. 184–191, IEEE, Columbus, OH, USA, June 2014.
\bibitem{} Hamza, R. M., \& Feyereisen, T. L. (2011). U.S. Patent No. 7,925,117. Washington, DC: U.S. Patent and Trademark Office.
\bibitem{} Gupta, A., Vedaldi, A., \& Zisserman, A. (2016). Synthetic data for text localisation in natural images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2315-2324).
\bibitem{} Holzer, S. J. J., Rusu, R. B., \& Ren, Y. (2017). U.S. Patent Application No. 15/425,988.
\bibitem{} Yan, X., Yang, J., Sohn, K., \& Lee, H. (2016, October). Attribute2image: Conditional image generation from visual attributes. In European Conference on Computer Vision (pp. 776-791). Springer, Cham.
\end{thebibliography}

\end{document}


